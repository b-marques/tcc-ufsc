%% chapters/chapter_1.tex
%%
%% Copyright 2017 Evandro Coan
%% Copyright 2012-2014 by abnTeX2 group at http://abntex2.googlecode.com/
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%%
%% The Current Maintainer of this work is the Evandro Coan.
%%
%% The last Maintainer of this work was the abnTeX2 team, led
%% by Lauro César Araujo. Further information are available on
%% https://www.abntex.net.br/
%%
%% This work consists of a bunch of files. But originally there were 2 files
%% which are renamed as follows:
%% Deleted the `abntex2-modelo-img-marca.pdf`
%% Renamed the `abntex2-modelo-include-comandos.tex, v-1.9.2 laurocesar` to `chapters/chapter_1.tex`
%%
% ---
% Este capítulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---

% The \phantomsection command is needed to create a link to a place in the document that is not a
% figure, equation, table, section, subsection, chapter, etc.
%
% When do I need to invoke \phantomsection?
% https://tex.stackexchange.com/questions/44088/when-do-i-need-to-invoke-phantomsection
\phantomsection


% Is it possible to keep my translation together with original text?
% https://tex.stackexchange.com/questions/5076/is-it-possible-to-keep-my-translation-together-with-original-text
\chapter{Fundamentação Teórica}
\label{cap:fundamentacao_teorica}

% \newpage

Este capítulo apresenta a fundamentação teórica sobre sistemas com múltiplos processadores, processador \manycore \mppa, padrão estêncil, \fw \pskel e a adapatação \pskelmppa.

\section{Sistemas com múltiplos processadores}
\label{sec:multicpu}

\textit{Tanenbaum}~\cite{Tanenbaum:2014:MOS:2655363} classifica sistemas com múltiplos processadores em dois grupos: os sistemas multiprocessadores e os multicomputadores. Nesta seção serão apresentados os principais conceitos dos sistemas multiprocessadores e multicomputadores.

\subsection{Multiprocessadores}
\label{subsec:multiprocessadores}

O multiprocessador é um sistema de computadores no qual qual duas ou mais \textit{Central Processing Units} (\cpus) compartilham acesso total a uma \textit{Random Access Memory} (\ram). Assim, programas executando em qualquer uma das \cpus realizam operações de leitura e escrita em um espaço de endereçamento físico único na \ram. A particularidade que este sistema apresenta é que, ao escrever um valor na memória, posteriormente ao ler este mesmo valor ele pode obter um resultado diferente daquele que foi escrito, pois outra \cpu pode alterá-lo. Percebe-se que este comportamento pode causar inconsistências no sistema e, por isso, é de extrema importância que recebam o tratamento adequado que, quando realizado, constitui a base para a comunicação entre processadores.

Apesar da propriedade de endereçamento total da memória, alguns multiprocessadores possuem a propriedade adicional de que cada palavra da memória pode ser lida tão rápido quanto a leitura de qualquer outra palavra presente na memória. Essas máquinas são denominadas multiprocessadores com acesso uniforme à memória -- \textit{Uniform Memory Access} (\uma). Outrossim, existem os multiprocessadores com acesso não uniforme à memória -- \textit{Nonuniform Memory Access} (\numa).

\subsubsection{Multiprocessadores UMA}
\label{subsubsec:hw-multiprocessadores-uma}

Os multiprocessadores UMA possuem variações nas suas arquiteturas. Os mais simples são os baseados em barramento, no qual o barramento é utilizado para comunicação. Desta maneira, quando uma \cpu deseja ler uma palavra da memória, será verificada a disponibilidade do barramento. Estando disponível, a \cpu informa o endereço no barramento e junto de alguns sinais de controle aguarda até que a memória coloque a palavra requisitada no barramento. Porém, quando o barramento está indisponível, a \cpu que deseja realizar leitura ou escrita da memória deve esperar até que o mesmo fique disponível. Quando existem poucas \cpus é possível realizar este gerenciamento, porém, quando o número de \cpus aumenta surge o problema, pois o sistema passa a possuir um gargalo no barramento, o que deixará muitas \cpus ociosas por muito tempo.

Uma maneira de reduzir o gargalo no barramento é através do uso de \textit{caches}/memória. Dessa maneira requisições satisfeitas pela \textit{cache} local não precisam ser realizadas através do barramento, desafogando o tráfego no mesmo e possibilitando o suporte a mais \cpus no sistema. Com o uso de \textit{caches} surge também a necessidade de estabelecer a coerência entre as \textit{caches} presentes no sistema. Para isso, são utilizados protocolos de coerência de \textit{cache}, que serão responsáveis por manter a memória e as \textit{caches} em um estado consistente.

\subsubsection{Multiprocessadores NUMA}
\label{subsubsec:hw-multiprocessadores-numa}

Mesmo com o melhor sistema de \textit{cache}, o uso de um único barramento limita o tamanho de um multiprocessador UMA para cerca de 16 ou 32 \cpus \cite{Tanenbaum:2014:MOS:2655363}. Para aumentar o número de \cpus, é necessário ir além e utilizar arquiteturas como barramento cruzado ou redes de comutação multi-estágio. Porém, estas arquiteturas necessitam de muito hardware, são caras, e não são tão maiores que as de barramento único. Para atingir a casa das dezenas de \cpus é necessário dispensar algo, normalmente o que é dispensado é o tempo de acesso uniforme para todos os módulos de memória. Isso resulta na ideia de multiprocessadores NUMA, que, assim como nos UMA, o espaço de endereçamento é único entre as \cpus, porém o tempo de acesso aos módulos de memória locais é mais rápido que o acesso aos módulos remotos.

Existem dois grupos entre os NUMA, os que fazem uso da coerência de cache \textit{Cache-Coherent NUMA} (CC-NUMA) e os que não fazem \textit{Non Cache-coherent NUMA} (NC-NUMA). Os sistemas NUMA possuem nós compostos por \cpu, \textit{cache}, dispositivos de \io e estão conectados por uma rede de interconexão.

\subsubsection{\textit{Chips} multinúcleo (\multicore)}
\label{subsubsec:chips-multinucleo}

Os avanços tecnológicos na fabricação de chips resultam em transistores cada vez menores, possibilitando o uso de um número maior destes em único \chip. Com tamanha quantidade de transistores em um único \chip é possível explorar os megabytes de \textit{cache} neles. Entretanto, existem limitações que fazem com que em um determinado ponto o aumento da \textit{cache} gere um ganho de apenas 0,5\% na taxa acerto \cite{Tanenbaum:2014:MOS:2655363}. Outra possibilidade é colocar múltiplas \cpus completas, denominadas normalmente de núcleos, em um mesmo \chip. Já são comuns \chips com até 8 núcleos e a presença de \textit{caches}, que são cruciais para o sistema, espalhadas pelo \chip.

\textit{Chips} multinúcleo são multiprocessadores muito pequenos, também são chamados de multiprocessadores em \chip~-- \textit{Chip MultiProcessors} (CMPs). Estes \chips são semelhantes aos multiprocessadores baseados em barramento ou redes de comunicação. O que os difere destes, é o fato das \cpus serem  conectadas muito próximas. Assim, falhas em componentes compartilhados podem acarretar na indisponibilidade de várias \cpus ao mesmo tempo, fato este que é improvável de ocorrer nos multiprocessadores tradicionais.

Também existem os sistemas em \chip~-- \textit{System on a Chip} (SoC), compostos de \cpus principais para processamento geral e de núcleos para tarefas específicas, como decodificação de aúdio e vídeo, criptoprocessadores, entre outros \cite{Tanenbaum:2014:MOS:2655363}.

\subsubsection{\textit{Chips} com muitos núcleos (\manycore)}
\label{subsubsec:chips-manycore}

Os \manycore são \multicore que contêm dezenas, centenas ou até milhares de núcleos. Embora não exista uma delimitação clara de quando um \multicore torna-se um \manycore, uma analogia utilizada é que um \multicore é um \manycore a partir do momento que a perda de um ou dois núcleos seja irrelevante \cite{Tanenbaum:2014:MOS:2655363}.

A presença de um elevado número de núcleos em um mesmo \chip acarreta em impactos sobre a coerência de \textit{cache}, principalmente quanto ao custo e complexidade dos protocolos de coerência, que atingem um limite a partir do qual ele passa a prejudicar o desempenho do sistema em questão. Devido a isso, processadores \textit{manycore} tendem a não utilizar \textit{caches} coerentes \cite{Tanenbaum:2014:MOS:2655363}.

A presença de milhares de núcleos não é mais algo tão incomum. As unidades de processamento gráfico -- \textit{Graphic Processing Unit} (\gpu), presentes em muitos sistemas computacionais de hoje em dia, são detentores de milhares de pequenos núcleos com enfoque no processamento de dados, e realizam muito menos operações de coerência de \textit{cache} e lógicas de controle que as \cpus de propósito geral. Estas mudanças estão refletidas na maneira de se programar para esses sistemas que, a exemplo das \gpus, utilizam linguagens específicas de programação, como \textit{Open Computing Language} (OpenCL) e CUDA, além de abordar a ideia de ``dados múltiplos e instrução única'', ou seja, a execução de uma mesma instrução de máquina sobre diferentes fragmentos de dados. 

\subsection{Multicomputadores}
\label{subsec:multicomputadores}

Devido ao alto custo decorrente da dificuldade de se produzir grandes multiprocessadores surgiram os multicomputadores, que são \cpus propriamente acopladas e que não compartilham memória. Estes são fáceis de se construir, já que necessita apenas de um computador sem periféricos munido de uma placa de interface de rede de alto desempenho. O desempenho do sistema será determinado pela inteligência do projeto de interconexão das \cpus e suas placas de \textit{interface}.

A necessidade de interconexão via rede implica em possibilidades de disposição dos nodos na rede, ou seja, possíveis topologias de rede conforme necessidade do sistema em questão, sendo elas topologia em anel, grade ou malha ( e sua variante toro duplo -- \textit{torus 2D}), topologia em cubo e hipercubo.

Na topologia em anel os nodos estão conecatados diretamente a dois outros nodos da rede, sendo um elemento à direita e um à esquerda. Nesta topologia a existência de um comutador é dispensada.

A topologia em grade ou malha, é constituída de uma malha bidimensional que interliga diversos nodos e possui uma distância máxima possível conhecida para percorrer um caminho de um nodo origem para um nodo destino. Uma variação desta topologia é a toro duplo, que possui estrutura semelhante à topologia em grade. Porém, suas margens são interconectadas, possibilitando com que margens opostas consigam se comunicar diretamente sem precisar passar por todos o nodos intermediários.

Além disso, existem as topologias tridimensionais, representadas pelo formato de cubo devido à forma que assumem. A partir desta pode-se alcançar topologias $n$-dimensionais bastanto replicar a topologia de uma dimensão de tamanho $n-1$ e ligar os nodos correspondentes, denominando-se topologia de hipercubo.

A comunicação em multicomputadores é realizada através da troca de mensagens utilizando rescursos disponibilizados pelo sistema operacional que podem ser reduzidos minimamente a duas chamadas de biblioteca, uma para enviar mensagens e outra para receber mensagens. A implementação destas bibliotecas de troca de mensagens possuem diversas variações de sistema para sistema. 

\section{MPPA-256}
\label{sec:mppa}

O \mppa é um processador \manycore desenvolvido pela empresa francesa Kalray. Apesar de ser considerado um \manycore ele apresenta características de multicomputador, pois seus núcleos estão conectados através de uma \noc.

Esse processador possui 256 núcleos de usuário e 32 núcleos de sistema para processamento a 400 MHz. Esses núcleos estão distribuídos entre 16 \textit{clusters} de computação e 4 \textit{clusters} de \textit{Input/Output}(\io), que se comunicam através de NoCs de dados e controle. O processador utilizado no desenvolver deste projeto possui uma memória global de baixa potência (LPDDR3) de 2GB conectada a um dos subsistemas de \io. A arquitetura do \mppa é ilustrada na Figura~\ref{fig:mppaOverall}. Cada cluster de computação tem os seguintes componentes:

\begin{itemize}
    \item 16 núcleos chamados de \pes (PEs), que são responsáveis por executar as \textit{threads} de usuário (uma \textit{thread} por \pe) e não pode ser interrompida ou preemptada;
    
    \item um \rman (RM), responsável por executar o sistema operacional e gerenciar a comunicação;
    
    \item uma memória compartilhada de baixa latência de 2MB, que permite uma grande banda e fluxo de dados e controle entre os \pes presentes no mesmo \textit{cluster} de computação; e
    
    \item dois controladores de NoC, um para dados e outro para controle.
    
\end{itemize}

\begin{figure}
    \centering
    \caption{Arquitetura do MPPA-256.}
    \includegraphics[width=0.75\textwidth]{figs/mppa-overview.pdf} \\
    \legend{Fonte: \cite{castro2013}}
    \label{fig:mppaOverall}
\end{figure}

O processador apresenta um modelo de memória distribuída. Os \clusters de computação e os \clusters de \io possuem espaço de endereçamento próprio. A exploração da computação paralela 
é realizada através de bibliotecas de código aberto, POSIX Threads (Pthreads) e OpenMP, e bibliotecas proprietárias, uma de baixo nível similar a POSIX \ipc e a \async, uma nova biblioteca disponibilizada pela Kalray e desenvolvida a partir da \ipc. As primeiras, visam o paralelismo de computação nos \clusters de computação através de memória compartilhada. Enquanto as últimas, seguem o modelo de memória distribuída e devem ser usadas para comunicação \cluster--\cluster e \cluster--\io via \noc.

A \async é baseada em comunicação unilateral entre a memória local dos \clusters de computação e a \lpddr. Os principais conceitos por trás da \async são domínios de execução, segmentos e operações de escrita e leitura. O domínio de execução representa um conjunto de núcleos compartilhando uma memória local, estando isolado de outros domínios de execução. Considerando o modelo de memória distribuída do \mppa, um domínio de execução corresponde a um \cluster de computação ou a um \cluster de \io. Memória que não é acessível diretamente a partir dos núcleos de um domínio de execução pode ser estruturada em segmentos, que correspondem a toda ou parte da memória local de núcleos localizados em outro domínio de execução. Cada segmento tem uma assinatura única, que é especificada quando o segmento é criado em um domínio de execução através da função \texttt{mppa\_async\_segment\_create()}. Então, outros domínios de execução podem referenciar um segmento previamente criado fornecendo a assinatura única para a função \texttt{mppa\_async\_segment\_clone()}. Assim que segmentos são criados e referenciados por diferentes domínios de execução, eles podem realizar operações de escrita de dados da memória local para um segmento remoto ou leitura de um segmento remoto para a memória local.
Diferentes funções para realizar estas operações estão disponíveis na biblioteca \async, permitindo transferências de dados contíguos ou espaçados (por exemplo, \texttt{mppa\_async\_put()} e \texttt{mppa\_async\_get\_spaced()}), assim como, transferências de blocos 2D (\texttt{mppa\_async\_sget\_block2d()}), que são úteis para transferir dados armazenados em estruturas bidimensionais.

O fluxo de execução de uma aplicação no \mppa ocorre da seguinte maneira: o processo principal (chamado processo mestre) executa em um \rman do \cluster de \io conectado à \lpddr e é responsável por alocar o dado de entrada na sua memória local (\lpddr) e inicializar os processos trabalhadores (uma para cada \cluster de computação) através da função \texttt{mppa\_power\_base\_spawn()}. Os segmentos de dados necessários devem ser criados pelo processo mestre para que a transferência de dados com os \clusters de computação aconteça. Por fim, o processo mestre deve esperar todos os trabalhadores terminarem através da função \texttt{mppa\_power\_base\_waitpid()}. Cada processo trabalhador deve referenciar o segmento remoto criado na \lpddr para realizar leitura e escrita de dados durante a execução, e pode criar até 16 \threads utilizando Pthread ou OpenMP (uma \thread para cada \pe) para computar paralelamente. Cada \pe tem sua \cache de memória privada sem qualquer mecanismo de coerência automático entre as outras \caches de memória dos \pes restantes. Embora isso melhore o desempenho da \cache, é necessário que o desenvolvedor realize o \textit{flush} explícito dos dados quando necessário. 

A biblioteca \ipc e seus detalhes serão retratados na \autoref{sec:pskel-mppa}, que aborda a atual implementação do \pskelmppa, que faz uso desta biblioteca.

Trabalhos anteriores mostraram que desenvolver aplicações paralelas otimizadas para o \mppa é um grande desafio~\cite{francesquini:hal-01092325} devido a alguns fatores importantes, tais como: o modelo de memória distribuída presente no \mppa, a capacidade de memória dentro do \textit{chip} e a comunicação explícita através da \noc. \textit{Podestá Jr.} \etal\citeyear{wscad2017} apresentam mais detalhes sobre esses desafios.


\section{Esqueletos paralelos}
\label{sec:esqueletosparalelos}

Um esqueleto paralelo é responsável por abstrair um determinado padrão de computação paralela. Para utilizar um esqueleto, o programador definirá as operações principais a serem realizadas, ou seja, definirá o \textit{kernel} da computação. Esse esqueleto será responsável em compor essa função definida pelo usuário, devendo ser executada de maneira correta, paralela e o mais eficiente possível.

A abstração de padrões paralelos contribui para a simplificação e desenvolvimento, além de reduzir o custo de modelagem, facilitar a transformação e otimização das computações.

Devido ao alto nível de abstração, esqueletos paralelos possuem alta afinidade com conceitos presentes nas linguagens de programação, como \textit{templates} e \textit{generics} das linguagens de programação orientada a objetos. Assim, os esqueletos paralelos exploram estes mecanismos \cite{Gorlatch2011}.

\subsection{Padrão estêncil}
\label{subsec:stencil}

Dentre os padrões presentes na literatura, o padrão estêncil é muito utilizado devido a sua aplicação nos campos de simulações meteorológicas, comportamento de fluídos físicos, entre outros.

Ilustrado pela Figura~\ref{fig:stencil}, funciona da seguinte forma: para cada elemento de uma estrutura $n$-dimensional é computado um novo valor relativo aos vizinhos do elemento atual. Os vizinhos de um elemento são determinados pela máscara da computação. Por fim, cada novo valor computado é atribuído à sua célula respectiva em uma estrutura $n$-dimensional de saída. Em aplicações estêncil iterativas, a estrutura de saída é utilizada como estrutura de entrada de uma nova iteração da aplicação.

\begin{figure}
    \centering
    \caption{O padrão estêncil.}
    \includegraphics[width=0.75\textwidth]{figs/stencilComputation.pdf} \\
    \legend{Fonte: \cite{wscad2017}}
    \label{fig:stencil}
\end{figure}

\section{\pskel}

O \pskel é um \fw de programação em alto nível para aplicações que utilizam o padrão estêncil, baseado no conceito de esqueletos paralelos, oferecendo suporte para a execução dessas aplicações em
ambientes heterogêneos, incluindo \cpu e \gpu. \pskel oferece um interface única de programação, desacoplada do \textit{back-end} de execução, permitindo que o usuário se preocupe apenas em implementar o \textit{kernel} estêncil que descreve a computação, enquanto o \fw fica responsável pela tradução das abstrações descritas para código paralelo de baixo nível em C++, gestão de memória e transferência de dados. Tudo isso de forma transparente para o usuário~\cite{CPE:CPE3479}.

O \autoref{code:pskel} mostra um exemplo do método Jacobi para resolver equações matriciais~\cite{demmel97} com o \pskel. A \api do \pskel fornece estruturas genéricas para manipular os dados de entrada e saída, chamadas \texttt{Array}, \texttt{Array2D} (\autoref{code:pskel}, linhas 12--13) e \texttt{Array3D}, através do uso de \textit{templates} da linguagem de programação C++. Essas abstrações fornecem métodos que encapsulam o gerenciamento dos dados, como alocação de memória, cópia de memória e transferência de dados entre \cpu e \gpu. Além disso, fornece abstrações para definição da computação estêncil e gerenciamento de sua execução. A computação estêncil é definida na função \texttt{stencilKernel()} (\autoref{code:pskel}, linhas 1--3), ela deve ser implementada pelo usuário do \pskel, é o método específico de cada aplicação, o qual descreve como a computação será realizada sobre cada elemento da matriz de entrada e seus vizinhos. O conjunto de classes para gerenciamento da execução são \texttt{Stencil}, \texttt{Stencil2D} (\autoref{code:pskel}, linha 15) e \texttt{Stencil3D}.

No exemplo fornecido, a função \texttt{stencilKernel()} será executada na GPU. O método \texttt{runIterativeGPU()} livra o usuário de escrever código específico na linguagem CUDA, necessário para excutar corretamente a computação estêncil na \gpu.

\begin{figure}
\begin{lstlisting}[
    caption={Trecho simplificado de código estêncil com \pskel.}, 
    label=code:pskel,
    language=C++,
    keywordstyle=\color{blue}\bfseries,
    otherkeywords = {__parallel__,struct,Array2D,Stencil2D}, 
    basicstyle=\tiny, 
    frame = single,]
__parallel__ void
stencilKernel(Array2D<float> A, Array2D<float> B, 
              struct Arguments args, int x, int y) {

    B(x,y) = args.alpha * (A(x,y+1) + A(x,y-1) + A(x+1,y) + A(x-1,y) 
             + args.beta);
}
        
void jacobi(float *A, float *B, int M, int N, float alpha, 
            float beta, int timesteps) {

    Array2D<float> input(A,M,N);
    Array2D<float> output(B,M,N);
    struct Arguments args(alpha, beta);
    Stencil2D<Array2D<float>, struct Arguments> stencil(input,output,args);
    stencil.runIterativeGPU(timesteps);
}
\end{lstlisting}
\end{figure}

\section{PSkel-MPPA}
\label{sec:pskel-mppa}

A adaptação PSkel-MPPA, é uma adaptação do \pskel proposta por~\cite{wscad2017}, ela faz uso de uma \api similar à POSIX \ipc para comunicação e será tratada como \ipc no decorrer deste trabalho. Nela, são utilizados portais de comunicação para o envio de dados e o método de \textit{strides}  para gerenciar explicitamente o envio e recebimento de \textit{tiles}.

\begin{figure}
    \centering
    \caption{Esquemático da proposta \pskelmppa.}
    \includegraphics[width=1\textwidth]{figs/visaoGeralPSKELMPPATrabalhador.pdf} \\
    \legend{Fonte - {\cite{Podesta:TCC}}}
    \label{fig:visaoGeralPSkelMPPA}
\end{figure}


A Figura~\ref{fig:visaoGeralPSkelMPPA} ilustra a adaptação
desenvolvida. É empregado o modelo mestre-trabalhador, que é um dos padrões de computação paralela utilizado quando existem múltiplos núcleos de processamento. O processo mestre é executado no \cluster de \io conectado a memória LPDDR3, aonde os dados de entrada e saída (os \emph{Array2Ds}) são alocados. Em seguida, ele calcula o número de \tiles alargados que serão produzidos, assim como suas dimensões baseado em: i) parametros definidos pelo usuário, como o tamanho da entrada de dados e as dimensões do \tile lógico, o número de \emph{clusters} de computação e o número de iterações internas; e ii) parâmetros do \emph{kernel} estêncil, como o tamanho da máscara. Então, são lançados até 16 processos trabalhadores (um em cada \cluster de computação) e é informado a cada processo trabalhador, o \tile pelo qual será responsável por processar na atual iteração. Cada processo trabalhador, por outro lado, aloca dados para armazenar o \tile alargado de entrada e de saída na memória local do \cluster de computação.

A fase de computação consiste da execução do \emph{kernel} estêncil pelos processos trabalhadores. As seguintes três principais etapas são realizadas para computar cada \tile atribuído a um processo trabalhador: 1) o \tile alargado é extraido do dado de entrada alocado na \lpddr e transferido para a memória local do \cluster de computação para ser processado, a extração e transferência é realizada com o uso de portais; 2) $t^\prime$ iterações do \emph{kernel} estêncil (iterações internas) são executadas pelo processo trabalhor sobre o \tile alargado, sincronizando com o processo mestre ao final das iterações internas, com o objetivo de receber um novo \textit{tile} ou para preparação dos dados a serem computados pelos trabalhadores nas iterações seguintes, ou término da execução; e 3). Ao alcançar o número total de iterações ($t$) definido pelo usuário, estará presente na \lpddr o \textit{Array2D} computado.

A comunicação realizada entre os processos mestre e trabalhadores é realizada por meio de portais, provenientes de uma \api proprietária de baixo nível do \mppa, similar à POSIX IPC. Os portais são responsáveis por realizar escrita e leitura de maneira remota, para isso um processo que deseja comunicar-se deve criar um portal com uma classificação (\textit{tag}) e relacioná-lo a um espaço de endereçamento, para posteriormente realizar leitura ou escrita no endereço associado.

A memória nos \textit{clusters} de computação (2MB) é limitada, devido a isso a matriz de entrada da computação (\textit{Array2D}) é particionada em tamanhos menores denominados \textit{tiles}, que possuem tamanho fixo definido pelo usuário para serem enviados aos \textit{clusters}.

As operações sobre \textit{tiles} e matrizes são realizadas sobre endereços de memória. Devido a restrição da \api e da \noc no \mppa, os dados armazenados em cada \textit{tile} precisam estar contíguos em memória para serem transferidos
pela \noc. Com o objetivo de evitar cópias locais de dados, utiliza-se o conceito de \textit{strides}. Cada \textit{stride} é uma parte contígua do \texttt{Array} original, sendo determinado por deslocamentos (\textit{offsets}) especificados durante a
execução. Os deslocamentos são dinâmicos e dependem dos \textit{tiles} sendo computados. Essas informações são conhecidas pelo processo mestre por meio da utilização da classe \texttt{StencilTiling}, e este as repassa aos processos trabalhadores \cite{Podesta:TCC}.

\begin{figure}[t]
	\centering
    \caption{Exemplo do funcionamento do método \textit{strides} no \mppa.}
	\includegraphics[width=0.9\textwidth]{figs/stridesImage.pdf} \\
    \legend{Fonte: {\cite{Podesta:TCC}}}
	\label{fig:strides}
\end{figure}


A Figura~\ref{fig:strides} ilustra o processo de comunicação com o método \textit{strides} para o caso de uma matriz de entrada de tamanho 6x6, \textit{tiles} de tamanho 3x3 e 4 \textit{clusters}. Ao computar todos os deslocamentos, o método irá enviar diretamente, via portal, os dados especificados pelo endereço inicial do \textit{tile} e pelos saltos na memória com a área útil especificada. Com isso, é possível enviar de forma contígua e direta os \textit{tiles} a outro processo, sem a necessidade de cópias locais \cite{Podesta:TCC}.

Quando se trata de particionamento de computação estêncil, é necessário tratar as depedências de vizinhança provenientes do padrão paralelo estêncil, antes de particionar os dados de entrada. A técnica de \textit{tiling} trapezoidal é utilizada para tratar as dependências de vizinhança, o que resulta na computação e presença redundante de dados~\cite{Rocha:2017}, porém em troca da presença de computação e dados redundantes, se reduz a quantidade de comunicações e sincronizações necessárias a serem realizadas, que se mostram extremamente prejudiciais para o desempenho. A definição formal está ilustrada a seguir.
Seja $A$ uma matriz de dados 2D, com dimensões $\textrm{dim}(A) = (w, h)$, aonde $w$ e $h$ são, respectivamente, sua largura e altura.
Utilizando \tiles de dimensões $(w^\prime, h^\prime)$ temos $\lceil\frac{w}{w^\prime}\rceil\lceil\frac{h}{h^\prime}\rceil$ como sendo os possíves \tiles de $A$.
Seja $A_{i,j}$ um único \tile, onde $0\leq i < \lceil\frac{w}{ w^\prime}\rceil$ e $0\leq j < \lceil\frac{h}{ h^\prime}\rceil$.
$A_{i,j}$ possui um deslocamento de $(i w^\prime,j h^\prime)$ em relação ao canto superior esquerdo de $A$ e $\textrm{dim}(A_{i,j}) = (\min\{w^\prime, w-i w^\prime\}, \min\{h^\prime, h-j h^\prime\})$.
Esse deslocamento atua como um indexador necessário para acessar os elementos de um \tile.

A Figura~\ref{fig:gputile} mostra a representação gráfica dessa técnica. Um \tile lógico (linha contínua interna) está contido em uma matriz de dados 2D (linha pontilhada externa) com deslocamento vertical e horizontal definidos por $j h^\prime$ e $i w^\prime$. Se $t$ iterações de uma aplicação estêncil devem ser executadas, é possível computar $t^\prime$ consecutivas iterações em $A_{i,j}$ ($t^\prime \in \left[1,t\right]$) se precisar de qualquer troca de dados entre \tiles adjacentes (também conheçido como iterações internas). Assim, o \tile lógico ($A_{i,j}$) precisa ser alargado, com o acréscimo de uma \emph{ghost zone} (área entre a linha sólida interna e externa), o que inclui a \emph{halo region} (a área entre a linha sólida interna e a linha pontilhada interna). Seja $r$ o mais distante deslocamento necessário para a vizinhança definida pela máscara estêncil. A área de alcance $r$ que define a vizinhança é denominada \textit{halo region}. O número de \emph{halo regions} adjacentes que compõe a \emph{ghost zone} é proporcional a $t^\prime$.
%
Assim, \tile alargado $A^\ast_{i,j}$ possui deslocamentos $(\max\{iw^\prime - rt^\prime, 0\}, \max\{jh^\prime - rt^\prime, 0\})$ relativos a $A$ \cite{Podesta:TCC}. 

\begin{figure}
	\centering
	\caption{Técnica de \textit{tiling} 2D.}
	\includegraphics[width=0.5\textwidth]{figs/tile.pdf} \\
	\legend{Fonte: {\cite{Rocha:2017}}}
	\label{fig:gputile}
\end{figure}

Felizmente, todas as complexas tarefas relacionas a técnica de \textit{tiling}, comunicação via \noc e adaptações discutidas nessa seção são transparentes para o usuário, tendo em vista que estão incluidas no \emph{back-end} do PSkelMPPA.